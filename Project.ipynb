{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0\n",
    "# All the needed imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Load the raw data\n",
    "\n",
    "original_df = pd.read_csv('ks-projects-201801.csv')\n",
    "\n",
    "# Remove all nans\n",
    "original_df = original_df.dropna()\n",
    "display(original_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Reorganize the data\n",
    "\n",
    "# Drop the following columns: ID, usd_pledged, usd_pledged_real, usd_goal_real\n",
    "df = original_df.drop(\n",
    "    [\"pledged\", \"usd pledged\", \"usd_pledged_real\", \"goal\", \"backers\"], axis=1\n",
    ")\n",
    "\n",
    "# Rename the columns\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"ID\": \"id\",\n",
    "        \"name\": \"name\",\n",
    "        \"category\": \"category\",\n",
    "        \"main_category\": \"main_category\",\n",
    "        \"deadline\": \"deadline\",\n",
    "        \"launched\": \"launched\",\n",
    "        \"state\": \"state\",\n",
    "        \"currency\": \"currency\",\n",
    "        \"country\": \"country\",\n",
    "        \"usd_goal_real\": \"goal\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Conver the launch and deadline to year-month-day\n",
    "df['launched'] = pd.to_datetime(pd.to_datetime(df['launched'], format=\"%Y-%m-%d %H:%M:%S\").dt.date)\n",
    "df['deadline'] = pd.to_datetime(pd.to_datetime(df['deadline'], format=\"%Y-%m-%d\").dt.date)\n",
    "\n",
    "# Calculate the amount of days between launch and deadline\n",
    "df['durration'] = (df['deadline'] - df['launched']).dt.days\n",
    "df['start_month'] = df['launched'].dt.month_name()\n",
    "df['end_month'] = df['deadline'].dt.month_name()\n",
    "df['start_day_name'] = df['launched'].dt.day_name()\n",
    "df['end_day_name'] = df['deadline'].dt.day_name()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print start shape\n",
    "print(df.shape)\n",
    "\n",
    "# Keep only success or fail\n",
    "df = df[(df['state'] == 'failed') | (df['state'] == 'successful')]\n",
    "\n",
    "# Remove country\n",
    "df = df[df['country'] != 'N,0\"']\n",
    "\n",
    "# df = df.drop(columns=['currency', 'country'], axis=1)\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Print end shape\n",
    "print(df.shape)\n",
    "\n",
    "# Final data before one hot encoding everything\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(df: pd.DataFrame, name):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    result = [] \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[name]\n",
    "        temp = sia.polarity_scores(text)\n",
    "        result.append([temp['neg'], temp['neu'], temp['pos'], temp['compound']])\n",
    "    # Generate column names\n",
    "    column_names = []\n",
    "    for value in ['negative', 'neutral', 'positive', 'compound']:\n",
    "        column_names.append(f\"{name}_{str(value)}\")\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    data_df = pd.DataFrame(np.array(result), columns=column_names)\n",
    "\n",
    "    # Return the final new DataFrame\n",
    "    # print(new_df.shape)\n",
    "    # print(data_df.dropna().shape)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    return pd.concat([df, data_df], axis=1)\n",
    "df = sentiment_analysis(df, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head(n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_column(df: pd.DataFrame, name):\n",
    "    # The existing column\n",
    "    column = df[name]\n",
    "\n",
    "    # Dataframe without the existing column\n",
    "    new_df = df.drop([name], axis=1)\n",
    "\n",
    "    # Get the unique values\n",
    "    unique = column.unique()\n",
    "    print(unique)\n",
    "\n",
    "    # Create a mapping from the unique value to the index\n",
    "    mapping = {key: index for index, key in enumerate(unique)}\n",
    "\n",
    "    # The encoded data\n",
    "    encoded = np.zeros((df.shape[0], len(unique)))\n",
    "\n",
    "    # Show mapping\n",
    "    # for key, index in mapping.items():\n",
    "    #     temp = np.zeros((len(unique)))\n",
    "    #     temp[index] = 1.0\n",
    "    #     print(f\"{temp}: {key}\")\n",
    "\n",
    "    # Encode each value\n",
    "    for offset, value in enumerate(column):\n",
    "        index = mapping[value]\n",
    "        encoded[offset][index] = 1\n",
    "\n",
    "    # Generate column names\n",
    "    column_names = []\n",
    "    for value in unique:\n",
    "        column_names.append(f\"{name}_{str(value)}\")\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    data_df = pd.DataFrame(encoded, columns=column_names, dtype=np.uint8)\n",
    "\n",
    "    # Return the final new DataFrame\n",
    "    # print(new_df.shape)\n",
    "    # print(data_df.dropna().shape)\n",
    "\n",
    "    new_df = new_df.reset_index(drop=True)\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    return pd.concat([new_df, data_df], axis=1)\n",
    "\n",
    "df = one_hot_encode_column(df, \"category\")\n",
    "df = one_hot_encode_column(df, \"main_category\")\n",
    "df = one_hot_encode_column(df, \"currency\")\n",
    "df = one_hot_encode_column(df, \"country\")\n",
    "df = one_hot_encode_column(df, \"start_month\")\n",
    "df = one_hot_encode_column(df, \"end_month\")\n",
    "df = one_hot_encode_column(df, \"start_day_name\")\n",
    "df = one_hot_encode_column(df, \"end_day_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df.copy()\n",
    "df_features = df_features.drop(columns=['name', 'state', 'deadline', 'launched'], axis=1)\n",
    "# Normalize goal and duration\n",
    "df_features['goal'] = (df_features['goal'] - df_features['goal'].mean()) / df_features['goal'].std()\n",
    "df_features['durration'] = (df_features['durration'] - df_features['durration'].mean()) / df_features['durration'].std()\n",
    "# df_features['start_year'] = (df_features['start_year'] - df_features['start_year'].mean()) / df_features['start_year'].std()\n",
    "# df_features['end_year'] = (df_features['end_year'] - df_features['end_year'].mean()) / df_features['end_year'].std()\n",
    "df_labels = df[['id', 'state']]\n",
    "df_labels['state'] = (df['state'] == 'successful').astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the min and max years\n",
    "display(df_features.head())\n",
    "print(df['launched'].dt.year.min())\n",
    "print(df['launched'].dt.year.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success_failure_summary(labels):\n",
    "    success_count = labels['state'].sum()\n",
    "    fail_count = labels.shape[0] - success_count \n",
    "    print(f\"# success: {success_count}\")\n",
    "    print(f\"# fail: {fail_count}\")\n",
    "    print(f\"Percent containin success: {round(success_count / labels.shape[0] * 100, 2)}\")\n",
    "    print(f\"Percent containin fail: {round(fail_count / labels.shape[0] * 100, 2)}\")\n",
    "success_failure_summary(df_labels)\n",
    "success_mask = df_labels['state'] == 1\n",
    "success_features = df_features[success_mask]\n",
    "success_labels = df_labels[success_mask]\n",
    "fail_mask = df_labels['state'] != 1\n",
    "fail_features = df_features[fail_mask]\n",
    "fail_labels = df_labels[fail_mask]\n",
    "min_length = min(success_mask.sum(), fail_mask.sum())\n",
    "success_features = success_features.head(min_length)\n",
    "success_labels = success_labels.head(min_length)\n",
    "fail_features = fail_features.head(min_length)\n",
    "fail_labels = fail_labels.head(min_length)\n",
    "final_labels = pd.concat([success_labels, fail_labels], axis=0)\n",
    "final_features =pd.concat([success_features, fail_features], axis=0)\n",
    "print(df_features.shape)\n",
    "print(final_features.shape)\n",
    "display(final_features.head())\n",
    "success_failure_summary(final_labels)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_features, df_labels, test_size=0.20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_features, final_labels, test_size=0.20)\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "display(X_train.head())\n",
    "display(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly Drop all ids\n",
    "final_X_train =  X_train.drop(columns=['id'], axis=1)\n",
    "final_X_test = X_test.drop(columns=['id'], axis=1)\n",
    "final_y_train = y_train.drop(columns=['id'], axis=1)\n",
    "final_y_test = y_test.drop(columns=['id'], axis=1)\n",
    "\n",
    "# PCA\n",
    "# X = final_X_train - np.mean(final_X_train, axis=0)\n",
    "# U, _, _ = np.linalg.svd(X.T, full_matrices=False)\n",
    "# G = U[:, :50]\n",
    "# final_X_train = np.dot(G.T, X.T).T\n",
    "# final_X_test = np.dot(G.T, (final_X_test - np.mean(final_X_test, axis=0)).T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PCA code\n",
    "knn = KNeighborsClassifier(n_neighbors=518, weights='distance', metric='l1')\n",
    "knn.fit(final_X_train, final_y_train.values.ravel())\n",
    "print(f\"{knn.predict(final_X_test[0:10])} vs {final_y_test['state'].values[0:10]}\")\n",
    "\n",
    "# Non-PCA code\n",
    "# knn.fit(final_X_train.values, final_y_train.values.ravel())\n",
    "# print(f\"{knn.predict(final_X_test[0:10])} vs {final_y_test['state'].values[0:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_test = knn.predict(final_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(final_y_train.shape)\n",
    "print(final_y_test.shape)\n",
    "\n",
    "print(\" --- Training Data ---\")\n",
    "success_failure_summary(y_train)\n",
    "print()\n",
    "\n",
    "print(\" --- Testing Data ---\")\n",
    "success_failure_summary(y_test)\n",
    "print()\n",
    "\n",
    "print(\" --- KNN Report ---\")\n",
    "print(classification_report(final_y_test.values.ravel(), predict_y_test))\n",
    "\n",
    "m = confusion_matrix(final_y_test.values.ravel(), predict_y_test)\n",
    "m = m / m.sum()\n",
    "ConfusionMatrixDisplay(m).plot()\n",
    "plt.show()\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs484venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
