{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0\n",
    "# All the needed imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Load the raw data\n",
    "\n",
    "original_df = pd.read_csv('ks-projects-201801.csv')\n",
    "\n",
    "# Remove all nans\n",
    "original_df = original_df.dropna()\n",
    "display(original_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Reorganize the data\n",
    "\n",
    "# Drop the following columns: ID, usd_pledged, usd_pledged_real, usd_goal_real\n",
    "df = original_df.drop(\n",
    "    [\"pledged\", \"usd pledged\", \"usd_pledged_real\", \"goal\", \"backers\"], axis=1\n",
    ")\n",
    "\n",
    "# Rename the columns\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"ID\": \"id\",\n",
    "        \"name\": \"name\",\n",
    "        \"category\": \"category\",\n",
    "        \"main_category\": \"main_category\",\n",
    "        \"deadline\": \"deadline\",\n",
    "        \"launched\": \"launched\",\n",
    "        \"state\": \"state\",\n",
    "        \"currency\": \"currency\",\n",
    "        \"country\": \"country\",\n",
    "        \"usd_goal_real\": \"goal\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert the launch and deadline to year-month-day\n",
    "df['launched'] = pd.to_datetime(pd.to_datetime(df['launched'], format=\"%Y-%m-%d %H:%M:%S\").dt.date)\n",
    "df['deadline'] = pd.to_datetime(pd.to_datetime(df['deadline'], format=\"%Y-%m-%d\").dt.date)\n",
    "\n",
    "# Calculate the amount of days between launch and deadline\n",
    "df['duration'] = (df['deadline'] - df['launched']).dt.days\n",
    "df['start_month'] = df['launched'].dt.month_name()\n",
    "df['end_month'] = df['deadline'].dt.month_name()\n",
    "df['start_year'] = df['launched'].dt.year\n",
    "df['end_year'] = df['deadline'].dt.year\n",
    "# df['start_day_name'] = df['launched'].dt.day_name()\n",
    "# df['end_day_name'] = df['deadline'].dt.day_name()\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Print start shape\n",
    "print(df.shape)\n",
    "\n",
    "# Keep only success or fail\n",
    "df = df[(df['state'] == 'failed') | (df['state'] == 'successful')]\n",
    "\n",
    "# Remove country\n",
    "df = df[df['country'] != 'N,0\"']\n",
    "\n",
    "# df = df.drop(columns=['currency', 'country'], axis=1)\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Print end shape\n",
    "print(df.shape)\n",
    "\n",
    "# Final data before one hot encoding everything\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "def sentiment_analysis(df: pd.DataFrame, name):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    result = [] \n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        text = row[name]\n",
    "        temp = sia.polarity_scores(text)\n",
    "        result.append([temp['neg'], temp['neu'], temp['pos'], temp['compound']])\n",
    "    # Generate column names\n",
    "    column_names = []\n",
    "    for value in ['negative', 'neutral', 'positive', 'compound']:\n",
    "        column_names.append(f\"{name}_{str(value)}\")\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    data_df = pd.DataFrame(np.array(result), columns=column_names)\n",
    "\n",
    "    # Return the final new DataFrame\n",
    "    # print(new_df.shape)\n",
    "    # print(data_df.dropna().shape)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    return pd.concat([df, data_df], axis=1)\n",
    "df = sentiment_analysis(df, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "display(df.head(n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "def one_hot_encode_column(df: pd.DataFrame, name):\n",
    "    # The existing column\n",
    "    column = df[name]\n",
    "\n",
    "    # Data frame without the existing column\n",
    "    new_df = df.drop([name], axis=1)\n",
    "\n",
    "    # Get the unique values\n",
    "    unique = column.unique()\n",
    "    print(unique)\n",
    "\n",
    "    # Create a mapping from the unique value to the index\n",
    "    mapping = {key: index for index, key in enumerate(unique)}\n",
    "\n",
    "    # The encoded data\n",
    "    encoded = np.zeros((df.shape[0], len(unique)))\n",
    "\n",
    "    # Show mapping\n",
    "    # for key, index in mapping.items():\n",
    "    #     temp = np.zeros((len(unique)))\n",
    "    #     temp[index] = 1.0\n",
    "    #     print(f\"{temp}: {key}\")\n",
    "\n",
    "    # Encode each value\n",
    "    for offset, value in enumerate(column):\n",
    "        index = mapping[value]\n",
    "        encoded[offset][index] = 1\n",
    "\n",
    "    # Generate column names\n",
    "    column_names = []\n",
    "    for value in unique:\n",
    "        column_names.append(f\"{name}_{str(value)}\")\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    data_df = pd.DataFrame(encoded, columns=column_names, dtype=np.uint8)\n",
    "\n",
    "    # Return the final new DataFrame\n",
    "    # print(new_df.shape)\n",
    "    # print(data_df.dropna().shape)\n",
    "\n",
    "    new_df = new_df.reset_index(drop=True)\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    return pd.concat([new_df, data_df], axis=1)\n",
    "\n",
    "df = one_hot_encode_column(df, \"category\")\n",
    "df = one_hot_encode_column(df, \"main_category\")\n",
    "df = one_hot_encode_column(df, \"currency\")\n",
    "df = one_hot_encode_column(df, \"country\")\n",
    "df = one_hot_encode_column(df, \"start_month\")\n",
    "df = one_hot_encode_column(df, \"end_month\")\n",
    "# df = one_hot_encode_column(df, \"start_day_name\")\n",
    "# df = one_hot_encode_column(df, \"end_day_name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "df_features = df.copy()\n",
    "df_features = df_features.drop(columns=[\n",
    "    # 'main_category',\n",
    "    # 'country',\n",
    "    # 'currency',\n",
    "    'name', 'state', 'deadline', 'launched'], axis=1)\n",
    "# Normalize goal and duration\n",
    "df_features['goal'] = (df_features['goal'] - df_features['goal'].mean()) / df_features['goal'].std()\n",
    "df_features['duration'] = (df_features['duration'] - df_features['duration'].mean()) / df_features['duration'].std()\n",
    "# df_features['duration'] = (df_features['duration'] - df_features['duration'].min()) / ( df_features['duration'].max() - df_features['duration'].min())\n",
    "df_features['start_year'] = (df_features['start_year'] - df_features['start_year'].min()) / ( df_features['start_year'].max() - df_features['start_year'].min())\n",
    "df_features['end_year'] = (df_features['end_year'] - df_features['end_year'].min()) / ( df_features['end_year'].max() - df_features['end_year'].min())\n",
    "# df_features['start_year'] = (df_features['start_year'] - df_features['start_year'].mean()) / df_features['start_year'].std()\n",
    "# df_features['end_year'] = (df_features['end_year'] - df_features['end_year'].mean()) / df_features['end_year'].std()\n",
    "df_labels = df[['id', 'state']]\n",
    "df_labels['state'] = (df['state'] == 'successful').astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Get the min and max years\n",
    "display(df_labels.head())\n",
    "display(df_features.head())\n",
    "print(df['launched'].dt.year.min())\n",
    "print(df['launched'].dt.year.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "def success_failure_summary(labels):\n",
    "    success_count = labels['state'].sum()\n",
    "    fail_count = labels.shape[0] - success_count \n",
    "    print(f\"# of success: {success_count}\")\n",
    "    print(f\"# of fail: {fail_count}\")\n",
    "    print(f\"Percent containing success: {round(success_count / labels.shape[0] * 100, 2)}%\")\n",
    "    print(f\"Percent containing fail: {round(fail_count / labels.shape[0] * 100, 2)}%\")\n",
    "\n",
    "print(\" -- Before --\")\n",
    "success_failure_summary(df_labels)\n",
    "print()\n",
    "\n",
    "# Get the successes\n",
    "success_mask = df_labels['state'] == 1\n",
    "success_features = df_features[success_mask]\n",
    "success_labels = df_labels[success_mask]\n",
    "\n",
    "# Get the failures\n",
    "fail_mask = df_labels['state'] != 1\n",
    "fail_features = df_features[fail_mask]\n",
    "fail_labels = df_labels[fail_mask]\n",
    "\n",
    "# Get the minimum length\n",
    "min_length = min(success_mask.sum(), fail_mask.sum())\n",
    "\n",
    "# Get the min length of successes and failures\n",
    "success_features = success_features.head(min_length)\n",
    "success_labels = success_labels.head(min_length)\n",
    "fail_features = fail_features.head(min_length)\n",
    "fail_labels = fail_labels.head(min_length)\n",
    "\n",
    "# Create final features and labels\n",
    "final_labels = pd.concat([success_labels, fail_labels], axis=0)\n",
    "final_features =pd.concat([success_features, fail_features], axis=0)\n",
    "\n",
    "# Uncomment if you do not want to even the success and failed\n",
    "# final_labels = df_labels\n",
    "# final_features = df_features\n",
    "\n",
    "print(df_features.shape)\n",
    "print(final_features.shape)\n",
    "display(final_features.head())\n",
    "\n",
    "print(\" -- After --\")\n",
    "success_failure_summary(final_labels)\n",
    "print()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(final_features, final_labels, test_size=0.20)\n",
    "# X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.05)\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_features, final_labels, test_size=0.20)\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "X_validation.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "y_validation.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\" -- Train --\")\n",
    "print(f\"Percent of data {round(X_train.shape[0] / final_features.shape[0] * 100, 3)}%\")\n",
    "success_failure_summary(y_train)\n",
    "print()\n",
    "\n",
    "print(\" -- Test --\")\n",
    "print(f\"Percent of data {round(X_test.shape[0] / final_features.shape[0] * 100, 3)}%\")\n",
    "success_failure_summary(y_test)\n",
    "print()\n",
    "\n",
    "print(\" -- Validation --\")\n",
    "print(f\"Percent of data {round(X_validation.shape[0] / final_features.shape[0] * 100, 3)}%\")\n",
    "success_failure_summary(y_validation)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Lastly Drop all ids\n",
    "final_X_train =  X_train.drop(columns=['id'], axis=1)\n",
    "final_X_test = X_test.drop(columns=['id'], axis=1)\n",
    "final_X_validation = X_validation.drop(columns=['id'], axis=1)\n",
    "final_y_train = y_train.drop(columns=['id'], axis=1)\n",
    "final_y_test = y_test.drop(columns=['id'], axis=1)\n",
    "final_y_validation = y_validation.drop(columns=['id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx, :], self.labels[idx]\n",
    "\n",
    "# Reset the index\n",
    "final_X_train.reset_index(inplace=True, drop=True)\n",
    "final_X_test.reset_index(inplace=True, drop=True)\n",
    "final_X_validation.reset_index(inplace=True, drop=True)\n",
    "final_y_test.reset_index(inplace=True, drop=True)\n",
    "final_y_train.reset_index(inplace=True, drop=True)\n",
    "final_y_validation.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Load up data into dataset\n",
    "training_data = OurDataset(final_X_train.values.astype(np.float32), final_y_train.values.ravel().astype(np.float32))\n",
    "test_data = OurDataset(final_X_test.values.astype(np.float32), final_y_test.values.ravel().astype(np.float32))\n",
    "validation_data = OurDataset(final_X_validation.values.astype(np.float32), final_y_validation.values.ravel().astype(np.float32))\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(f\"Shape of X [N, F]: {X.shape} {X.dtype}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "# Get cpu or gpu device for training.\n",
    "import torch\n",
    "from torch import nn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "feature_count = final_X_train.shape[1]\n",
    "\n",
    "# Define model\n",
    "class ComplexNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        size = 3 * feature_count\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(feature_count, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return torch.flatten(logits)\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        size = 2 * feature_count\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(feature_count, feature_count),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(feature_count, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.layers(x)\n",
    "        return torch.flatten(logits)\n",
    "\n",
    "simple_model = NeuralNetwork().to(device)\n",
    "print(\"--- Simple ---\")\n",
    "print(simple_model)\n",
    "\n",
    "complex_model = ComplexNeuralNetwork().to(device)\n",
    "print(\"--- Complex ---\")\n",
    "print(complex_model)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(simple_model.parameters(), lr=1e-3, weight_decay=1e-4, momentum=0.9)\n",
    "complex_optimizer = torch.optim.AdamW(complex_model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "def test(dataloader, model, loss_fn, name=\"Test\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"{name} Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    print(\"Simple:\")\n",
    "    train(train_dataloader, simple_model, loss_fn, optimizer)\n",
    "    test(test_dataloader, simple_model, loss_fn)\n",
    "    print(\"Complex:\")\n",
    "    train(train_dataloader, complex_model, loss_fn, complex_optimizer)\n",
    "    test(test_dataloader, complex_model, loss_fn)\n",
    "print(\"Done!\")\n",
    "print(\"Simple:\")\n",
    "test(validation_dataloader, simple_model, loss_fn, name=\"Validation\")\n",
    "print(\"Complex:\")\n",
    "test(validation_dataloader, complex_model, loss_fn, name=\"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "predict_y_validation = simple_model(torch.tensor(final_X_validation.values.astype(np.float32)).to(device))\n",
    "predict_y_validation = predict_y_validation.round().to('cpu').detach().numpy()\n",
    "\n",
    "print(predict_y_validation.shape)\n",
    "print(\" --- Simple NN Report ---\")\n",
    "print(classification_report(final_y_validation.values.ravel(), predict_y_validation))\n",
    "\n",
    "m = confusion_matrix(final_y_validation.values.ravel(), predict_y_validation)\n",
    "m = m / m.sum()\n",
    "ConfusionMatrixDisplay(m).plot()\n",
    "plt.show()\n",
    "\n",
    "predict_y_validation = complex_model(torch.tensor(final_X_validation.values.astype(np.float32)).to(device))\n",
    "predict_y_validation = predict_y_validation.round().to('cpu').detach().numpy()\n",
    "\n",
    "print(predict_y_validation.shape)\n",
    "print(\" --- Complex NN Report ---\")\n",
    "print(classification_report(final_y_validation.values.ravel(), predict_y_validation))\n",
    "\n",
    "m = confusion_matrix(final_y_validation.values.ravel(), predict_y_validation)\n",
    "m = m / m.sum()\n",
    "ConfusionMatrixDisplay(m).plot()\n",
    "plt.show()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "class RandomModel:\n",
    "    def __init__(self) -> None:\n",
    "        None\n",
    "    def predict(self, values):\n",
    "        result = np.random.random((values.shape[0], 1)).round()\n",
    "        return result\n",
    "\n",
    "    def predict_proba(self, values):\n",
    "        result = np.zeros((values.shape[0], 2))\n",
    "        for offset in range(values.shape[0]):\n",
    "            result[offset] = [0.5, 0.5]\n",
    "        return result\n",
    "\n",
    "random_model = RandomModel()\n",
    "predict_y_validation = random_model.predict(final_X_validation)\n",
    "print(predict_y_validation.shape)\n",
    "\n",
    "print(predict_y_validation.shape)\n",
    "print(\" --- Random Report ---\")\n",
    "print(classification_report(final_y_validation.values.ravel(), predict_y_validation))\n",
    "\n",
    "m = confusion_matrix(final_y_validation.values.ravel(), predict_y_validation)\n",
    "m = m / m.sum()\n",
    "ConfusionMatrixDisplay(m).plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "uniform_knn = KNeighborsClassifier(n_neighbors=518, weights='uniform', metric='l1')\n",
    "uniform_knn.fit(final_X_train.values, final_y_train.values.ravel())\n",
    "\n",
    "distance_knn = KNeighborsClassifier(n_neighbors=518, weights='distance', metric='l1')\n",
    "distance_knn.fit(final_X_train.values, final_y_train.values.ravel())\n",
    "\n",
    "predict_y_validation = uniform_knn.predict(final_X_validation.values)\n",
    "print(predict_y_validation.shape)\n",
    "print(\" --- (uniform) KNN Report ---\")\n",
    "print(classification_report(final_y_validation.values.ravel(), predict_y_validation))\n",
    "\n",
    "m = confusion_matrix(final_y_validation.values.ravel(), predict_y_validation)\n",
    "m = m / m.sum()\n",
    "ConfusionMatrixDisplay(m).plot()\n",
    "plt.show()\n",
    "\n",
    "predict_y_validation = distance_knn.predict(final_X_validation.values)\n",
    "print(predict_y_validation.shape)\n",
    "print(\" --- (distance) KNN Report ---\")\n",
    "print(classification_report(final_y_validation.values.ravel(), predict_y_validation))\n",
    "\n",
    "m = confusion_matrix(final_y_validation.values.ravel(), predict_y_validation)\n",
    "m = m / m.sum()\n",
    "ConfusionMatrixDisplay(m).plot()\n",
    "plt.show()\n",
    "None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "import lime\n",
    "from lime import lime_tabular\n",
    "\n",
    "explainer = lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(final_X_train),\n",
    "    feature_names=final_X_train.columns,\n",
    "    class_names=['failed', 'successful'],\n",
    "    mode='classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17\n",
    "y_zero_index = None\n",
    "y_one_index = None\n",
    "skip_zero = 0\n",
    "skip_one = 0\n",
    "for index, y in enumerate(final_y_validation.values):\n",
    "    y_expected = y[0]\n",
    "    x = np.array([ final_X_validation.iloc[index]])\n",
    "    simple_nn_y = simple_model(torch.tensor(x.astype(np.float32)).to(device))\n",
    "    simple_nn_y = int(simple_nn_y.round().to('cpu').detach().numpy()[0])\n",
    "    complex_nn_y = complex_model(torch.tensor(x.astype(np.float32)).to(device))\n",
    "    complex_nn_y = int(complex_nn_y.round().to('cpu').detach().numpy()[0])\n",
    "    uniform_knn_y = uniform_knn.predict(x)[0]\n",
    "    distance_knn_y = distance_knn.predict(x)[0]\n",
    "    values = [\n",
    "        simple_nn_y,\n",
    "        complex_nn_y,\n",
    "        uniform_knn_y,\n",
    "        distance_knn_y\n",
    "    ]\n",
    "    different = False\n",
    "    for value in values:\n",
    "        if value != y_expected:\n",
    "            different = True\n",
    "            break\n",
    "    if not different:\n",
    "        if y_expected == 0 and y_zero_index is None:\n",
    "            if skip_zero > 0:\n",
    "                skip_zero -= 1\n",
    "            else:\n",
    "                print(index, y_expected, values)\n",
    "                y_zero_index = index\n",
    "        elif y_expected == 1 and y_one_index is None:\n",
    "            if skip_one > 0:\n",
    "                skip_one -= 1\n",
    "            else:\n",
    "                print(index, y_expected, values)\n",
    "                y_one_index = index\n",
    "        if y_one_index is not None and y_zero_index is not None:\n",
    "            break\n",
    "assert y_zero_index is not None\n",
    "assert y_one_index is not None\n",
    "\n",
    "def complex_func(x):\n",
    "    predict_y = complex_model(torch.tensor(x.astype(np.float32)).to(device))\n",
    "    predict_y = 1 - predict_y.to('cpu').detach().numpy()\n",
    "    opposite = 1- predict_y\n",
    "    result = np.atleast_2d(predict_y).T\n",
    "    result = np.append(result, np.atleast_2d(opposite).T, axis=1)\n",
    "    return result\n",
    "\n",
    "def simple_func(x):\n",
    "    predict_y = simple_model(torch.tensor(x.astype(np.float32)).to(device))\n",
    "    predict_y = 1 - predict_y.to('cpu').detach().numpy()\n",
    "    opposite = 1- predict_y\n",
    "    result = np.atleast_2d(predict_y).T\n",
    "    result = np.append(result, np.atleast_2d(opposite).T, axis=1)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_zero_index]),\n",
    "    predict_fn=random_model.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_zero_index]),\n",
    "    predict_fn=uniform_knn.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_zero_index]),\n",
    "    predict_fn=distance_knn.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_zero_index]),\n",
    "    predict_fn=simple_func,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_zero_index]),\n",
    "    predict_fn=complex_func,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_one_index]),\n",
    "    predict_fn=random_model.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_one_index]),\n",
    "    predict_fn=uniform_knn.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_one_index]),\n",
    "    predict_fn=distance_knn.predict_proba,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_one_index]),\n",
    "    predict_fn=simple_func,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    data_row=np.array(final_X_validation.iloc[y_one_index]),\n",
    "    predict_fn=complex_func,\n",
    "    num_features=10,\n",
    "    num_samples=10000\n",
    ")\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width: 90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
